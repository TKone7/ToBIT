
@article{dembsey_closing_2017,
	title = {Closing the {Grammarly}® {Gaps}: {A} {Study} of {Claims} and {Feedback} from an {Online} {Grammar} {Program}},
	volume = {36},
	issn = {0889-6143},
	shorttitle = {Closing the {Grammarly}® {Gaps}},
	url = {https://www.jstor.org/stable/44252638},
	abstract = {From 2012 to 2015, the online grammar program Grammarly® was claimed to complement writing center services by 1. increasing student access to writing support; and 2. addressing sentence-level issues, such as grammar. To test if Grammarly® could close these two gaps in writing center services, this article revisits the results of a Spring 2014 study that compared Grammarly®'s comment cards to the written feedback of 10 asynchronous online consultants. The results showed that both Grammarly® and some consultants strayed from effective practices regarding limiting feedback, avoiding technical language, and providing accurate information about grammatical structure. However, the consultants' weaknesses could be addressed with enhanced or focused training, and their strengths allowed for important learning opportunities that enable student access to information across mediums and help students establish connections between their sentences and the larger whole. This article concludes that each writing center should consider their own way of closing these gaps and offers suggestions for multiple consultation genres, new services, and strategies for sentence-level concerns.},
	number = {1},
	urldate = {2019-10-14},
	journal = {The Writing Center Journal},
	author = {Dembsey, J. M.},
	year = {2017},
	pages = {63--100},
	annote = {Closing the Grammarly® Gaps: A Study of Claims and Feedback from an Online Grammar Program},
	annote = {Tests if the gaps could be closed.
Compares result with feedback of 10 real consultants.},
	file = {Dembsey - 2017 - Closing the Grammarly® Gaps A Study of Claims and.pdf:/home/tobias/Zotero/storage/K8CBJF4P/Dembsey - 2017 - Closing the Grammarly® Gaps A Study of Claims and.pdf:application/pdf}
}

@patent{hoover_united_2015,
	title = {United {States} {Patent}: 9002700 - {Systems} and methods for advanced grammar checking},
	shorttitle = {United {States} {Patent}},
	url = {http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=10&f=G&l=50&co1=AND&d=PTXT&s1=9,002,700&OS=9,002,700&RS=9,002,700},
	abstract = {In embodiments of the present invention improved capabilities are
     described for methods and systems of grammar checking comprising a
     grammar checking facility and a plurality of human proofreaders in a
     crowd-source population for the detection and correction of grammatical
     errors in text as received from a computing device with input
     restrictions including reduced size keyboard and display.},
	assignee = {Grammarly, Inc.},
	number = {9002700},
	urldate = {2019-10-14},
	author = {Hoover, Bradley and Lytvyn, Maksym and CA and Shevchenko, Oleksiy and CA},
	month = apr,
	year = {2015}
}

@article{nadasdi_anything_2007,
	title = {Anything {I} can do, {CPU} can do better: {A} comparison of human and computer grammar correction for {L}2 writing using {BonPatron}. com},
	volume = {1},
	shorttitle = {Anything {I} can do, {CPU} can do better},
	journal = {Unpublished manuscript. Retrieved Sept},
	author = {Nadasdi, Terry and Sinclair, Stéfan},
	year = {2007},
	pages = {2010},
	annote = {Focuses on french - Will not use at all
look at it (webpage)
Section with commercial offerings},
	file = {Full Text:/home/tobias/Zotero/storage/6TX4F4JD/Nadasdi and Sinclair - 2007 - Anything I can do, CPU can do better A comparison.pdf:application/pdf}
}

@article{burstein_automated_2004,
	title = {Automated {Essay} {Evaluation}: {The} {Criterion} {Online} {Writing} {Service}},
	volume = {25},
	copyright = {Copyright (c)},
	issn = {2371-9621},
	shorttitle = {Automated {Essay} {Evaluation}},
	url = {https://www.aaai.org/ojs/index.php/aimagazine/article/view/1774},
	doi = {10.1609/aimag.v25i3.1774},
	abstract = {In this article, we describe a deployed educational technology application: the Criterion Online Essay Evaluation Service, a web-based system that provides automated scoring and evaluation of student essays. Criterion has two complementary applications: (1) CritiqueWriting Analysis Tools, a suite of programs that detect errors in grammar, usage, and mechanics, that identify discourse elements in the essay, and that recognize potentially undesirable elements of style, and (2) e-rater version 2.0, an automated essay scoring system. Critique and e-rater provide students with feedback that is specific to their writing in order to help them improve their writing skills and is intended to be used under the instruction of a classroom teacher. Both applications employ natural language processing and machine learning techniques. All of these capabilities outperform baseline algorithms, and some of the tools agree with human judges in their evaluations as often as two judges agree with each other.},
	language = {en},
	number = {3},
	urldate = {2019-10-14},
	journal = {AI Magazine},
	author = {Burstein, Jill and Chodorow, Martin and Leacock, Claudia},
	month = sep,
	year = {2004},
	pages = {27--27},
	file = {Full Text PDF:/home/tobias/Zotero/storage/U5TGLYTI/Burstein et al. - 2004 - Automated Essay Evaluation The Criterion Online W.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/XLUETRW4/1774.html:text/html}
}

@article{kohut_effectiveness_1995,
	title = {The {Effectiveness} of {Leading} {Grammar}/{Style} {Software} {Packages} in {Analyzing} {Business} {Students}' {Writing}},
	volume = {9},
	issn = {1050-6519},
	url = {https://doi.org/10.1177/1050651995009003004},
	doi = {10.1177/1050651995009003004},
	abstract = {This study compares the effectiveness of five leading grammar/style analysis software packages in analyzing business students' writing. The software exhibited considerable differences in the following areas: correctly identifying various mechanical and style errors, avoiding annoying and misleading false error messages, and providing helpful remedial advice. No prior research study has empirically compared grammar/style analysis software along all these important dimensions. PowerEdit was found to be the overall superior package, demonsrating proficiency in detecting errors in punctuation, sentence structure, passive voice, and weak wording. The results have significant implications for utilizing grammar/style analysis software to improve students' writing.},
	language = {en},
	number = {3},
	urldate = {2019-10-14},
	journal = {Journal of Business and Technical Communication},
	author = {KOHUT, GARY F. and GORMAN, KEVIN J.},
	month = jul,
	year = {1995},
	pages = {341--361},
	annote = {very old
 }
}

@article{vojak_new_2011,
	title = {New {Spaces} and {Old} {Places}: {An} {Analysis} of {Writing} {Assessment} {Software}},
	volume = {28},
	issn = {8755-4615},
	shorttitle = {New {Spaces} and {Old} {Places}},
	url = {http://www.sciencedirect.com/science/article/pii/S8755461511000259},
	doi = {10.1016/j.compcom.2011.04.004},
	abstract = {This article examines the strengths and weaknesses of emerging writing assessment technologies. Instead of providing a comprehensive review of each program, we take a deliberately selective approach using three key understandings about writing as a framework for analysis: writing is a socially situated activity; writing is functionally and formally diverse; and writing is a meaning-making activity that can be conveyed in multiple modalities. We conclude that the programs available today largely neglect the potential of emerging technologies to promote a broader vision of writing. Instead, they tend to align with the narrow view of writing dominant in a more recent era of testing and accountability, a view that is increasingly thrown into question. New technologies, we conclude, are for the most part being used to reinforce old practices. At a time when computer technology is increasingly looked to as a way to improve assessment, these findings have important implications.},
	number = {2},
	urldate = {2019-10-14},
	journal = {Computers and Composition},
	author = {Vojak, Colleen and Kline, Sonia and Cope, Bill and McCarthey, Sarah and Kalantzis, Mary},
	month = jun,
	year = {2011},
	keywords = {Assessment, Learner feedback, Pedagogy, Technology, Writing},
	pages = {97--111},
	file = {Full Text:/home/tobias/Zotero/storage/E2W48C4U/Vojak et al. - 2011 - New Spaces and Old Places An Analysis of Writing .pdf:application/pdf;ScienceDirect Snapshot:/home/tobias/Zotero/storage/KKK5ZYV6/S8755461511000259.html:text/html}
}

@article{warschauer_automated_2006,
	title = {Automated writing evaluation: defining the classroom research agenda},
	volume = {10},
	issn = {1362-1688},
	shorttitle = {Automated writing evaluation},
	url = {https://doi.org/10.1191/1362168806lr190oa},
	doi = {10.1191/1362168806lr190oa},
	abstract = {With the advent of English as a global language, the ability to write well in English across diverse settings and for different audiences has become an imperative in second language education programmes throughout the world. Yet the teaching of second language writing is often hindered by the great amount of time and skill needed to evaluate repeated drafts of student writing. Online Automated Writing Evaluation programmes have been developed as a way to meet this challenge, and the scoring engines driving such programmes have been analysed in a considerable array of psychometric studies. However, relatively little research has been conducted on how AWE is used in the classroom and the results achieved with such use. In this article, we analyse recent developments in automated writing evaluation, explain the bases on which AWE systems operate, synthesize research with these systems, and propose a multifaceted process/product research programme on the instructional use of AWE. We explore this emerging area of inquiry by proposing a range of potential questions, methodologies and analytical tools that can define such a research agenda.},
	language = {en},
	number = {2},
	urldate = {2019-10-14},
	journal = {Language Teaching Research},
	author = {Warschauer, Mark and Ware, Paige},
	month = apr,
	year = {2006},
	pages = {157--180},
	annote = {maybe related},
	file = {Warschauer and Ware - 2006 - Automated writing evaluation defining the classro.pdf:/home/tobias/Zotero/storage/46Y8NFAR/Warschauer and Ware - 2006 - Automated writing evaluation defining the classro.pdf:application/pdf}
}

@patent{fein_method_2000,
	title = {Method and system for background grammar checking an electronic document},
	url = {https://patents.google.com/patent/US6012075A/en},
	nationality = {US},
	assignee = {Microsoft Corp},
	number = {US6012075A},
	urldate = {2019-10-14},
	author = {Fein, Ronald A. and Krueger, Anthony D.},
	month = jan,
	year = {2000},
	keywords = {document, error, flag, grammar, sentence},
	file = {Fulltext PDF:/home/tobias/Zotero/storage/M8UE98AQ/Fein and Krueger - 2000 - Method and system for background grammar checking .pdf:application/pdf}
}

@inproceedings{burstein_toward_2003,
	address = {Stroudsburg, PA, USA},
	series = {{EACL} '03},
	title = {Toward {Evaluation} of {Writing} {Style}: {Finding} {Overly} {Repetitive} {Word} {Use} in {Student} {Essays}},
	shorttitle = {Toward {Evaluation} of {Writing} {Style}},
	url = {https://doi.org/10.3115/1067807.1067814},
	doi = {10.3115/1067807.1067814},
	abstract = {Automated essay scoring is now an established capability used from elementary school through graduate school for purposes of instruction and assessment. Newer applications provide automated diagnostic feedback about student writing. Feedback includes errors in grammar, usage, and mechanics, comments about writing style, and evaluation of discourse structure. This paper reports on a system that evaluates a characteristic of lower quality essay writing style: repetitious word use. This capability is embedded in a commercial writing assessment application, CriterionSM. The system uses a machine-learning approach with word-based features to model repetitious word use in an essay. System performance well exceeds several baseline algorithms. Agreement between the system and a single human judge exceeds agreement between two human judges.},
	urldate = {2019-10-14},
	booktitle = {Proceedings of the {Tenth} {Conference} on {European} {Chapter} of the {Association} for {Computational} {Linguistics} - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Burstein, Jill and Wolska, Magdalena},
	year = {2003},
	note = {event-place: Budapest, Hungary},
	pages = {35--42},
	annote = {interesting
 },
	file = {ACM Full Text PDF:/home/tobias/Zotero/storage/IBT5A6H8/Burstein and Wolska - 2003 - Toward Evaluation of Writing Style Finding Overly.pdf:application/pdf}
}

@inproceedings{patout_towards_2019,
	address = {New York, NY, USA},
	series = {{EASEAI} 2019},
	title = {Towards {Context}-aware {Automated} {Writing} {Evaluation} {Systems}},
	isbn = {978-1-4503-6852-0},
	url = {http://doi.acm.org/10.1145/3340435.3342722},
	doi = {10.1145/3340435.3342722},
	abstract = {Writing is a crucial skill in our society, which is regularly exerted by students across all disciplines. Automated essay scoring and automatic writing evaluation systems can support professors in the evaluation of written texts and, conversely, help students improving their writing. However, most of those systems fail to consider the context of the writing, such as the targeted audience and the genre. In this paper, we depict our vision towards new-generation AES systems that could evaluate written products while considering their specific context. In education, such tools could support students not only in adapting their written product to their particular context, but also in identifying points for improvement and situational settings where their writing is less proficient.},
	urldate = {2019-10-14},
	booktitle = {Proceedings of the 1st {ACM} {SIGSOFT} {International} {Workshop} on {Education} {Through} {Advanced} {Software} {Engineering} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Patout, Pierre-André and Cordy, Maxime},
	year = {2019},
	note = {event-place: Tallinn, Estonia},
	keywords = {automated writing evaluation, context awareness, education},
	pages = {17--20},
	annote = {rescent important},
	file = {Full Text:/home/tobias/Zotero/storage/NR2INFVJ/Patout and Cordy - 2019 - Towards Context-aware Automated Writing Evaluation.pdf:application/pdf}
}

@inproceedings{wang_case_2011,
	title = {A {Case} {Study} on the {Efficacy} of {Error} {Correction} {Practice} by {Using} the {Automated} {Writing} {Evaluation} {System} {WRM} 2.0 on {Chinese} {College} {Students}' {English} {Writing}},
	doi = {10.1109/ICCIS.2011.21},
	abstract = {A case study was conducted to get insights into the efficacy of error correction practice by using the automated writing evaluation system Writing Roadmap2.0 (WRM), on Chinese college students' English writing. In the research, a group of fresh students in Southwest Petroleum University were required to finish a title- and outline-given essay within 30 minutes by using WRM, and then to revise the essays according to the electronic feedback provided by the system in 15 minutes. By comparing and analyzing the feedback reports given by the system, we found that the participants enjoyed better self-editing ability, writing accuracy and writing quality, but no distinct changes in the organization and ideas. The research indicated that students were able to revise their essays effectively according to the feedback information provided by WRM, but they needed more guidance and advice from teachers if they wanted to improve the structure, coherence and cohesion. This study offers new evidence to support the practice by using automated evaluation system on pedagogical grounds.},
	booktitle = {2011 {International} {Conference} on {Computational} and {Information} {Sciences}},
	author = {Wang, S. and Xian, Y.},
	month = oct,
	year = {2011},
	keywords = {Accuracy, automated evaluation system, case study, Correlation, Educational institutions, English writing quality, feedback, Organizations, Petroleum, self-revising ability, Writing},
	pages = {988--991},
	file = {Full Text:/home/tobias/Zotero/storage/Z9YT7SJU/Wang and Xian - 2011 - A Case Study on the Efficacy of Error Correction P.pdf:application/pdf;IEEE Xplore Abstract Record:/home/tobias/Zotero/storage/Z5N92SME/6086368.html:text/html}
}

@inproceedings{oakman_evolution_1994,
	title = {The evolution of intelligent writing assistants: trends and future prospects},
	shorttitle = {The evolution of intelligent writing assistants},
	doi = {10.1109/TAI.1994.346488},
	abstract = {Since Writer's Workbench (Bell Telephone Laboratories, early 1980's), software for writing assistance and style checking has evolved over the last decade (1984-94) to become more intelligent and interactive. During this period the author has been involved in the development of several software packages and has monitored the growth and sophistication of software solutions. Today certain characteristics have become standard; yet challenges remain. The author discusses trends and suggests areas where we might expect continued future development. Today writers have a variety of useful but limited style and grammar checkers available for most computer systems. Some even come bundled with the current generation of enhanced word processors and are used by many writers. Another approach to writing assistance is suggested by online interactive group writing systems like MediaLink. Possibilities for combining the best features of current systems exist, but further improvements in the quality of the knowledge offered by automated writing assistants will depend on research advances in other areas of natural language processing. The author examines some of these problem areas and suggests approaches from ongoing NLP research that we can expect the writing assistants and style checkers of the future to include among their resources.{\textless}{\textgreater}},
	booktitle = {Proceedings {Sixth} {International} {Conference} on {Tools} with {Artificial} {Intelligence}. {TAI} 94},
	author = {Oakman, R. L.},
	month = nov,
	year = {1994},
	keywords = {Collaborative work, Computer science, enhanced word processors, future prospects, grammar checkers, intelligent writing assistants, knowledge based systems, Laboratories, MediaLink, Microcomputers, Monitoring, multimedia computing, natural language interfaces, natural language processing, NLP research, online interactive group writing systems, Pattern matching, research advances, software packages, Software packages, style checkers, style checking, Table lookup, Telephony, word processing, Writing, writing assistance},
	pages = {233--234},
	file = {IEEE Xplore Abstract Record:/home/tobias/Zotero/storage/WKBG36ZC/metrics.html:text/html}
}

@inproceedings{rubens_solving_2004,
	title = {Solving writing issues related to non-native writers of {English}},
	doi = {10.1109/IPCC.2004.1375272},
	abstract = {Non-native writers of English often have considerable difficulty in creating English language texts. This presents even more problems for scientists and engineers who prepare texts for other subject-area specialists. Typically, solutions to these problems have considered only the nuts-and-bolts approach of 'fixing " the grammar issues. However, the problems are certainly farther reaching in scope, especially for writers of scientific and technical texts. Such issues as tone, specialized terminology, paragraph logic, quantifiers and measurement systems, the writing process, and style and grammar all need to be considered. This work presents the results of several years research involving both academic and professional non-native writers; in doing so, we offer some useful suggestions to help this audience achieve better communication results.},
	booktitle = {International {Professional} {Communication} {Conference}, 2004. {IPCC} 2004. {Proceedings}.},
	author = {Rubens, P. and Southard, S.},
	month = sep,
	year = {2004},
	keywords = {DNA, Education, English language text, grammar issues, grammars, Logic, measurement systems, Natural languages, nonnative English writers, paragraph logic, Particle measurements, Professional activities, professional communication, Publishing, scientific texts, specialized terminology, subject-area specialists, technical texts, Terminology, Writing, writing instruction, writing issues, writing process},
	pages = {42--46},
	annote = {Not directly to do with automatic correction. But we can still learn how to improve.},
	annote = {see if its relevant
 },
	file = {Full Text:/home/tobias/Zotero/storage/5TIZPYYF/Rubens and Southard - 2004 - Solving writing issues related to non-native write.pdf:application/pdf;IEEE Xplore Abstract Record:/home/tobias/Zotero/storage/86TKKIWK/1375272.html:text/html}
}

@article{dodigovic_artificial_2007,
	title = {Artificial {Intelligence} and {Second} {Language} {Learning}: {An} {Efficient} {Approach} to {Error} {Remediation}},
	volume = {16},
	issn = {0965-8416},
	shorttitle = {Artificial {Intelligence} and {Second} {Language} {Learning}},
	url = {https://doi.org/10.2167/la416.0},
	doi = {10.2167/la416.0},
	abstract = {While theoretical approaches to error correction vary in the second language acquisition (SLA) literature, most sources agree that such correction is useful and leads to learning. While some point out the relevance of the communicative context in which the correction takes place, others stress the value of consciousness-raising. Trying to reconcile the two approaches, this paper describes an application of artificial intelligence in the second language error remediation process. The software presented is called the Intelligent Tutor. It diagnoses some typical errors in the writing of university students who are learning English as a second language. A quasi-experimental study consisting of a grammaticality judgment pre-test, a treatment in the form of the Intelligent Tutor and a short answer post-test, was carried out with 266 university students in three countries. The findings show that artificial intelligence is an efficient instrument of error remediation, reducing the error rate by an average of 83\%. This paper discusses the theoretical underpinnings of the software used, briefly describes the software itself and then presents the study design, its findings and their implications in the wider context of second language learning.},
	number = {2},
	urldate = {2019-10-14},
	journal = {Language Awareness},
	author = {Dodigovic, Marina},
	month = may,
	year = {2007},
	keywords = {artificial intelligence, error correction, Intelligent Tutor, remediation, second language learning},
	pages = {99--113},
	annote = {how correction can help to learn
how to design a system},
	file = {Full Text:/home/tobias/Zotero/storage/QUFUX7MT/Dodigovic - 2007 - Artificial Intelligence and Second Language Learni.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/RVJXC4V5/la416.html:text/html}
}

@article{li_rethinking_2015,
	title = {Rethinking the role of automated writing evaluation ({AWE}) feedback in {ESL} writing instruction},
	volume = {27},
	issn = {1060-3743},
	url = {http://www.sciencedirect.com/science/article/pii/S1060374314000757},
	doi = {10.1016/j.jslw.2014.10.004},
	abstract = {The development of language processing technologies and statistical methods has enabled modern automated writing evaluation (AWE) systems to provide feedback on language and content in addition to an automated score. However, concerns have been raised with regard to the instructional and assessment value of AWE in writing classrooms. The findings from a few classroom-based studies concerning the impact of AWE on writing instruction and performance are largely inconclusive. Meanwhile, since research provides favorable evidence for the reliability of AWE corrective feedback, and that writing accuracy is both an important and frustrating issue, it is worthwhile to examine more specifically the impact of AWE corrective feedback on writing accuracy. Therefore, the study used mixed-methods to investigate how Criterion® affected writing instruction and performance. Results suggested that Criterion® has led to increased revisions, and that the corrective feedback from Criterion® helped improve accuracy from a rough to a final draft. The potential benefits were also confirmed by the instructors’ interviews. The students’ perspectives were mixed, but the extent to which the views vary may depend on the students’ language proficiency level and their instructors’ use and perspectives of AWE.},
	urldate = {2019-10-14},
	journal = {Journal of Second Language Writing},
	author = {Li, Jinrong and Link, Stephanie and Hegelheimer, Volker},
	month = mar,
	year = {2015},
	keywords = {AWE, Corrective feedback, ESL writing, Mixed-methods research},
	pages = {1--18},
	file = {Full Text:/home/tobias/Zotero/storage/SEA6JHNI/Li et al. - 2015 - Rethinking the role of automated writing evaluatio.pdf:application/pdf;ScienceDirect Snapshot:/home/tobias/Zotero/storage/DUZQZTFA/S1060374314000757.html:text/html}
}

@article{wang_exploring_2013,
	title = {Exploring the impact of using automated writing evaluation in {English} as a foreign language university students' writing},
	volume = {26},
	issn = {0958-8221},
	url = {https://doi.org/10.1080/09588221.2012.655300},
	doi = {10.1080/09588221.2012.655300},
	abstract = {A period of expanding globalization has emphasized the significant role played by the English language in both communications and information sourcing. In such an environment, it is understandable that many have sought to enhance available writing skills and the assessment of said writing skills. In recent decades, automated writing evaluation (AWE) has been applied with significant frequency to the evaluation and assessment of English writing performance in EFL environments. The purpose of this study is to examine the impact and effect of using AWE on EFL students' writing. In this study, the subjects were 57 EFL freshmen from the Department of Applied English at one university in the south of Taiwan. Both quantitative and qualitative research methods were used to conduct a quasi-experimental research by employing a t-test technique and a semi-structured interview technique, and the nonequivalent groups' pre-test/post-test control and comparison group design were applied to explore the overall effect of using AWE on the improvement of student writing in terms of accuracy, learner autonomy, and interaction. The research results reveal a significant difference between the experimental group and the control group in terms of writing accuracy following the adoption of AWE. Regarding the overall effect and the exploration of students' perceptions toward their usage of the AWE software, it shows that students who used AWE display obvious writing enhancement in terms of writing accuracy and learner autonomy awareness. The pedagogical implications of fully understanding students' usage of AWE in the writing process in order to improve EFL learners' writing performance are fully discussed.},
	number = {3},
	urldate = {2019-10-14},
	journal = {Computer Assisted Language Learning},
	author = {Wang, Ying-Jian and Shang, Hui-Fang and Briody, Paul},
	month = jul,
	year = {2013},
	keywords = {accuracy, automated writing evaluation (AWE), EFL writing, interaction, learner autonomy},
	pages = {234--257},
	annote = {bad title},
	file = {Full Text:/home/tobias/Zotero/storage/GYIYLKLM/Wang et al. - 2013 - Exploring the impact of using automated writing ev.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/PTR4L4RR/09588221.2012.html:text/html}
}

@article{chen_beyond_2008,
	title = {Beyond the design of automated writing evaluation: {Pedagogical} practices and perceived learning effectiveness in {EFL} writing classes},
	volume = {12},
	shorttitle = {Beyond the design of automated writing evaluation},
	number = {2},
	journal = {Language Learning \& Technology},
	author = {Chen, Chi-Fen Emily and Cheng, Wei-Yuan Eugene Cheng},
	year = {2008},
	pages = {94--112},
	file = {Full Text:/home/tobias/Zotero/storage/KCNGAS4P/Chen and Cheng - 2008 - Beyond the design of automated writing evaluation.pdf:application/pdf}
}

@article{grimes_utility_2010,
	title = {Utility in a {Fallible} {Tool}: {A} {Multi}-{Site} {Case} {Study} of {Automated} {Writing} {Evaluation}},
	volume = {8},
	copyright = {Copyright (c)},
	issn = {1540-2525},
	shorttitle = {Utility in a {Fallible} {Tool}},
	url = {https://ejournals.bc.edu/index.php/jtla/article/view/1625},
	abstract = {Automated writing evaluation (AWE) software uses artificial intelligence (AI) to score student essays and support revision. We studied how an AWE program called MY Access!® was used in eight middle schools in Southern California over a three-year period. Although many teachers and students considered automated scoring unreliable, and teachers’ use of AWE was limited by the desire to use conventional writing methods, use of the software still brought important benefits. Observations, interviews, and a survey indicated that using AWE simplified classroom management and increased students’ motivation to write and revise.},
	language = {en},
	number = {6},
	urldate = {2019-10-14},
	journal = {The Journal of Technology, Learning and Assessment},
	author = {Grimes, Douglas and Warschauer, Mark},
	month = mar,
	year = {2010},
	keywords = {artifical intelligence, automated essay scoring, automated writing evaluation, composition, writing},
	file = {Full Text PDF:/home/tobias/Zotero/storage/9J4DUWGR/Grimes and Warschauer - 2010 - Utility in a Fallible Tool A Multi-Site Case Stud.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/5MZXU8AM/1625.html:text/html}
}

@incollection{shermis_automated_2016,
	address = {New York, NY, US},
	title = {Automated writing evaluation: {An} expanding body of knowledge},
	isbn = {978-1-4625-2243-9 978-1-4625-2245-3},
	shorttitle = {Automated writing evaluation},
	abstract = {The chapter on machine scoring for the first edition of this Handbook focused almost exclusively on the use of automated writing evaluation (AWE) technologies to provide information about writing ability in summative assessment contexts. Since that time, AWE has developed its own body of knowledge, building out from its origins in natural language processing and socio cognitive approaches to construct modeling (Shermis, Burstein, \& Bursky, 2013). To demonstrate the depth of contemporary knowledge about AWE, in this chapter we present AWE in terms of the categories of evidence used to demonstrate that these systems are useful in the instruction and assessment of writing. Defining the documentation of such usefulness in terms of validation (Kane, 2006), Williamson, Xi, and Breyer (2012) proposed a unique conceptual framework for AWE focusing on explanation (construct, task, and scoring investigation); evaluation (human and automated score relationships); generalization (generalizability across alternate tasks and test forms); extrapolation (external measure relationships); and utilization (score use and consequences). In an adaptation and reuse of their framework, we discuss the AWE body of knowledge as follows: The Explanation section offers descriptions of how four different AWE systems provide information that may be used to draw relationships between systems and construct relevance; the Evaluation section provides descriptions of the scoring and evaluation functions of AWE systems; the Generalization section discusses empirical studies that use AWE to assess student performance; the Extrapolation section argues for the development of AWE systems that can evaluate writing task types beyond the genre of the academic essay; and the Utilization section focuses on the application of AWE in the context of decision making. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
	booktitle = {Handbook of writing research, 2nd ed},
	publisher = {The Guilford Press},
	author = {Shermis, Mark D. and Burstein, Jill and Elliot, Norbert and Miel, Shayne and Foltz, Peter W.},
	year = {2016},
	keywords = {Computerized Assessment, Scoring (Testing), Writing Skills},
	pages = {395--409},
	annote = {maybe related / not available},
	file = {Snapshot:/home/tobias/Zotero/storage/HCAWJPCF/2015-51875-026.html:text/html}
}

@article{cotos_potential_2011,
	title = {Potential of {Automated} {Writing} {Evaluation} {Feedback}},
	volume = {28},
	issn = {0742-7778},
	url = {https://www.jstor.org/stable/calicojournal.28.2.420},
	abstract = {ABSTRACT This paper presents an empirical evaluation of automated writing evaluation (AWE) feedback used for L2 academic writing teaching and learning. It introduces the Intelligent Academic Discourse Evaluator (IADE), a new web-based AWE program that analyzes the introduction section to research articles and generates immediate, individualized, and discipline-specific feedback. The purpose of the study was to investigate the potential of IADE's feedback. A mixed-methods approach with a concurrent transformative strategy was employed. Quantitative data consisted of responses to Likert-scale, yes/no, and open-ended survey questions; automated and human scores for first and final drafts; and pre-/posttest scores. Qualitative data contained students' first and final drafts as well as transcripts of think-aloud protocols and Camtasia computer screen recordings, observations, and semistructured interviews. The findings indicate that IADE's colorcoded and numerical feedback possesses potential for facilitating language learning, a claim supported by evidence of focus on discourse form, noticing of negative evidence, improved rhetorical quality of writing, and increased learning gains.},
	number = {2},
	urldate = {2019-10-14},
	journal = {CALICO Journal},
	author = {Cotos, Elena},
	year = {2011},
	pages = {420--459},
	file = {Cotos - 2011 - Potential of Automated Writing Evaluation Feedback.pdf:/home/tobias/Zotero/storage/HK2AU7AR/Cotos - 2011 - Potential of Automated Writing Evaluation Feedback.pdf:application/pdf}
}

@article{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and signiﬁcantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	language = {en},
	urldate = {2019-10-14},
	journal = {arXiv:1802.05365 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05365},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready},
	annote = {from Ela
 },
	file = {Peters et al. - 2018 - Deep contextualized word representations.pdf:/home/tobias/Zotero/storage/RAW2Y62R/Peters et al. - 2018 - Deep contextualized word representations.pdf:application/pdf}
}

@article{bell_context_2019,
	title = {Context is {Key}: {Grammatical} {Error} {Detection} with {Contextual} {Word} {Representations}},
	shorttitle = {Context is {Key}},
	url = {http://arxiv.org/abs/1906.06593},
	abstract = {Grammatical error detection (GED) in nonnative writing requires systems to identify a wide range of errors in text written by language learners. Error detection as a purely supervised task can be challenging, as GED datasets are limited in size and the label distributions are highly imbalanced. Contextualized word representations offer a possible solution, as they can efﬁciently capture compositional information in language and can be optimized on large amounts of unsupervised data. In this paper, we perform a systematic comparison of ELMo, BERT and Flair embeddings (Peters et al., 2017; Devlin et al., 2018; Akbik et al., 2018) on a range of public GED datasets, and propose an approach to effectively integrate such representations in current methods, achieving a new state of the art on GED. We further analyze the strengths and weaknesses of different contextual embeddings for the task at hand, and present detailed analyses of their impact on different types of errors.},
	language = {en},
	urldate = {2019-10-14},
	journal = {arXiv:1906.06593 [cs]},
	author = {Bell, Samuel and Yannakoudakis, Helen and Rei, Marek},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.06593},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {from Ela},
	file = {Bell et al. - 2019 - Context is Key Grammatical Error Detection with C.pdf:/home/tobias/Zotero/storage/TQI8T9XS/Bell et al. - 2019 - Context is Key Grammatical Error Detection with C.pdf:application/pdf}
}

@article{cavaleri_you_2016,
	title = {You want me to check your grammar again? {The} usefulness of an online grammar checker as perceived by students},
	volume = {10},
	shorttitle = {You want me to check your grammar again?},
	number = {1},
	journal = {Journal of Academic Language and Learning},
	author = {Cavaleri, Michelle Rose and Dianati, Saib},
	year = {2016},
	pages = {A223--A236},
	annote = {overview of Grammarly
trial at two colleges
doing survey according to usefulness
also reviews the acceptance by students},
	file = {Cavaleri and Dianati - 2016 - You want me to check your grammar again The usefu.pdf:/home/tobias/Zotero/storage/DK2JJ7IG/Cavaleri and Dianati - 2016 - You want me to check your grammar again The usefu.pdf:application/pdf}
}

@article{manchanda_various_2016,
	title = {Various techniques used for grammar checking},
	volume = {9},
	number = {1},
	journal = {International Journal of Computer Applications \& Information Technology},
	author = {Manchanda, Blossom and Athavale, Vijay Anant and kumar Sharma, Sanjeev},
	year = {2016},
	pages = {177},
	annote = {comparison of three different techniques (rule basec, statisctica, syntax)
 },
	file = {Full Text:/home/tobias/Zotero/storage/7VWZMZQ8/Manchanda et al. - 2016 - Various techniques used for grammar checking.pdf:application/pdf}
}

@article{qassemzadeh_impact_2016,
	title = {The {Impact} of {Feedback} {Provision} by {Grammarly} {Software} and {Teachers} on {Learning} {Passive} {Structures} by {Iranian} {EFL} {Learners}},
	volume = {6},
	copyright = {Copyright (c) 2016 Academy Publication},
	issn = {1799-2591},
	url = {http://www.academypublication.com/ojs/index.php/tpls/article/view/tpls060918841894},
	doi = {10.17507/tpls.0609.23},
	abstract = {A major concern in today's world of pedagogy in general and language teaching, in particular, is the application of computer-assisted learning to improve students' achievement. There has been a long time that in the classroom setting only the teacher's feedback in a traditional way has been used in teaching. Due to the fact that this kind of notion can be traced back to a traditional attitude toward feedback, we looked for a new alternative in order to bring some innovation in an educational environment, namely Grammarly Software feedback provision. Therefore, the aim of the present study was to explore the impact of feedback provision by Grammarly Software and teachers on learning passive structures by EFL learners. Through convenience sampling, 70 intermediate male and female EFL learners were selected, then they were randomly assigned to two main groups: the experimental and control group. A grammar pre-test, a post-test, and a delayed post-test were administrated to the participants in six sessions. The results of the data gathered from pre-test and post-test reveal that the effect of teacher on learning passive structure, in pre-test and post-test, were more than the effect of Grammarly Software on learning passive structure of the learners, and the effect of Grammarly Software on learning passive structure in delayed post-test scores was more than the effect of teacher on learning passive structure of learners. The results might have implications for language teachers, learners, and materials developers.},
	language = {en},
	number = {9},
	urldate = {2019-10-23},
	journal = {Theory and Practice in Language Studies},
	author = {Qassemzadeh, Abolfazl and Soleimani, Hassan},
	month = sep,
	year = {2016},
	keywords = {grammarly software, passive structure, software feedback, teacher's feedback},
	pages = {1884--1894},
	annote = {quite useless
 },
	file = {Full Text PDF:/home/tobias/Zotero/storage/VU3LGRMN/Qassemzadeh and Soleimani - 2016 - The Impact of Feedback Provision by Grammarly Soft.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/8ABS9QQX/tpls060918841894.html:text/html}
}

@article{nova_utilizing_2018,
	title = {{UTILIZING} {GRAMMARLY} {IN} {EVALUATING} {ACADEMIC} {WRITING}: {A} {NARRATIVE} {RESEARCH} {ON} {EFL} {STUDENTS}’ {EXPERIENCE}},
	volume = {7},
	copyright = {Copyright (c) 2018 Muhamad Nova},
	issn = {2442-482X},
	shorttitle = {{UTILIZING} {GRAMMARLY} {IN} {EVALUATING} {ACADEMIC} {WRITING}},
	url = {http://ojs.fkip.ummetro.ac.id/index.php/english/article/view/1332},
	doi = {10.24127/pj.v7i1.1332},
	abstract = {With the development of technology, any writer now can easily check their academic writing with automated writing evaluation program. Though, the utilization of this program may bring both benefits and drawbacks. Thus, a consideration of its strengths and weaknesses is needed. To fill the need, this study aimed to identify the strengths and weaknesses of Grammarly program as an automated writing evaluation program in evaluating academic writing. Using a narrative inquiry in exploring three Indonesian postgraduate students’ experiences by conducting interview and documentation, the result showed that this program has provided useful color-coded feedback with explanation and example, ease of account access, high rate of evaluation speed, and free service for evaluating academic writing. However, some caveats were also found in this program utilization, such as several misleading feedbacks, weaknesses on detecting the type of English and reference list, and lack of context and content evaluation experienced, which became the weaknesses of this program. Further investigation on the efficiency of the feedback given by Grammarly in improving students’ writing quality is needed.},
	language = {en},
	number = {1},
	urldate = {2019-10-23},
	journal = {Premise: Journal of English Education},
	author = {Nova, Muhamad},
	month = apr,
	year = {2018},
	keywords = {academic writing, automated writing evaluation, experience, Grammarly},
	pages = {80--97},
	file = {Full Text PDF:/home/tobias/Zotero/storage/C444MJV5/Nova - 2018 - UTILIZING GRAMMARLY IN EVALUATING ACADEMIC WRITING.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/UIWABZT4/1332.html:text/html}
}

@techreport{ventayen_graduate_2018,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Graduate {Students}’ {Perspective} on the {Usability} of {Grammarly}® in {One} {ASEAN} {State} {University}},
	url = {https://papers.ssrn.com/abstract=3310702},
	abstract = {One of the criteria for excellent work is a perfect grammar in English, which is the global lingua franca. The Pangasinan State University subscribed to Grammarly® software which considered as one of the leading grammar checker and plagiarism tester software available. The objective of the paper is to evaluate the software based on the perception of the graduate students from the Graduate Students of PSU Open University Systems. The selection of participants is purposive where 20\% of the total graduate students of the Batch 2018 who completed there thesis writing before graduation was selected as the respondents. A SUS questionnaire and follow up the interview as part of the triangulation method was used in order to determine the usability of the software, its strengths, and weakness. This study also focuses on the impact of the application for Non-English majors. Based on the result of the study, the majority of the respondents agree that the utilized software is usable. The users identified the strength of the software that helps the user improve writing such as automatic detection of mistakes in Conceptual Writing, Grammar, Punctuation, Sentence Structure, Style and Vocabulary Enhancement. While other users also identified some weakness for possible future improvement of the software, the overall result shows that there is a significant improvement in writing for English and non-English majors. Students who are not English majors showed that there is a significant change of confidence level in writing. It recommends that the software is continuously utilized. It is also recommended that the software should improve its detection to avoid misleading feedback for users.},
	language = {en},
	number = {ID 3310702},
	urldate = {2019-10-23},
	institution = {Social Science Research Network},
	author = {Ventayen, Randy Joy Magno and Orlanda-Ventayen, Caren C.},
	month = aug,
	year = {2018},
	keywords = {grammar checker, grammarly, plagiarism test},
	file = {Snapshot:/home/tobias/Zotero/storage/3BKVPHZX/papers.html:text/html;Ventayen and Orlanda-Ventayen - 2018 - Graduate Students’ Perspective on the Usability of.pdf:/home/tobias/Zotero/storage/NETC3E9F/Ventayen and Orlanda-Ventayen - 2018 - Graduate Students’ Perspective on the Usability of.pdf:application/pdf}
}

@misc{noauthor_write_nodate,
	title = {Write your best with {Grammarly}.},
	url = {https://www.grammarly.com/},
	abstract = {Grammarly makes sure everything you type is easy to read, effective, and mistake-free. Try it today:},
	urldate = {2019-11-27},
	file = {Snapshot:/home/tobias/Zotero/storage/D36J3T9Y/www.grammarly.com.html:text/html}
}