
@article{dembsey_closing_2017,
	title = {Closing the {Grammarly}® {Gaps}: {A} {Study} of {Claims} and {Feedback} from an {Online} {Grammar} {Program}},
	volume = {36},
	issn = {0889-6143},
	shorttitle = {Closing the {Grammarly}® {Gaps}},
	url = {https://www.jstor.org/stable/44252638},
	abstract = {From 2012 to 2015, the online grammar program Grammarly® was claimed to complement writing center services by 1. increasing student access to writing support; and 2. addressing sentence-level issues, such as grammar. To test if Grammarly® could close these two gaps in writing center services, this article revisits the results of a Spring 2014 study that compared Grammarly®'s comment cards to the written feedback of 10 asynchronous online consultants. The results showed that both Grammarly® and some consultants strayed from effective practices regarding limiting feedback, avoiding technical language, and providing accurate information about grammatical structure. However, the consultants' weaknesses could be addressed with enhanced or focused training, and their strengths allowed for important learning opportunities that enable student access to information across mediums and help students establish connections between their sentences and the larger whole. This article concludes that each writing center should consider their own way of closing these gaps and offers suggestions for multiple consultation genres, new services, and strategies for sentence-level concerns.},
	number = {1},
	urldate = {2019-10-14},
	journal = {The Writing Center Journal},
	author = {Dembsey, J. M.},
	year = {2017},
	pages = {63--100},
	annote = {Closing the Grammarly® Gaps: A Study of Claims and Feedback from an Online Grammar Program},
	annote = {Tests if the gaps could be closed.
Compares result with feedback of 10 real consultants.},
	file = {Dembsey - 2017 - Closing the Grammarly® Gaps A Study of Claims and.pdf:/home/tobias/Zotero/storage/K8CBJF4P/Dembsey - 2017 - Closing the Grammarly® Gaps A Study of Claims and.pdf:application/pdf}
}

@patent{hoover_united_2015,
	title = {United {States} {Patent}: 9002700 - {Systems} and methods for advanced grammar checking},
	shorttitle = {United {States} {Patent}},
	url = {http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=10&f=G&l=50&co1=AND&d=PTXT&s1=9,002,700&OS=9,002,700&RS=9,002,700},
	abstract = {In embodiments of the present invention improved capabilities are
     described for methods and systems of grammar checking comprising a
     grammar checking facility and a plurality of human proofreaders in a
     crowd-source population for the detection and correction of grammatical
     errors in text as received from a computing device with input
     restrictions including reduced size keyboard and display.},
	assignee = {Grammarly, Inc.},
	number = {9002700},
	urldate = {2019-10-14},
	author = {Hoover, Bradley and Lytvyn, Maksym and CA and Shevchenko, Oleksiy and CA},
	month = apr,
	year = {2015}
}

@article{nadasdi_anything_2007,
	title = {Anything {I} can do, {CPU} can do better: {A} comparison of human and computer grammar correction for {L}2 writing using {BonPatron}. com},
	volume = {1},
	shorttitle = {Anything {I} can do, {CPU} can do better},
	journal = {Unpublished manuscript. Retrieved Sept},
	author = {Nadasdi, Terry and Sinclair, Stéfan},
	year = {2007},
	pages = {2010},
	annote = {Focuses on french - Will not use at all
look at it (webpage)
Section with commercial offerings},
	file = {Full Text:/home/tobias/Zotero/storage/6TX4F4JD/Nadasdi and Sinclair - 2007 - Anything I can do, CPU can do better A comparison.pdf:application/pdf}
}

@article{burstein_automated_2004,
	title = {Automated {Essay} {Evaluation}: {The} {Criterion} {Online} {Writing} {Service}},
	volume = {25},
	copyright = {Copyright (c)},
	issn = {2371-9621},
	shorttitle = {Automated {Essay} {Evaluation}},
	url = {https://www.aaai.org/ojs/index.php/aimagazine/article/view/1774},
	doi = {10.1609/aimag.v25i3.1774},
	abstract = {In this article, we describe a deployed educational technology application: the Criterion Online Essay Evaluation Service, a web-based system that provides automated scoring and evaluation of student essays. Criterion has two complementary applications: (1) CritiqueWriting Analysis Tools, a suite of programs that detect errors in grammar, usage, and mechanics, that identify discourse elements in the essay, and that recognize potentially undesirable elements of style, and (2) e-rater version 2.0, an automated essay scoring system. Critique and e-rater provide students with feedback that is specific to their writing in order to help them improve their writing skills and is intended to be used under the instruction of a classroom teacher. Both applications employ natural language processing and machine learning techniques. All of these capabilities outperform baseline algorithms, and some of the tools agree with human judges in their evaluations as often as two judges agree with each other.},
	language = {en},
	number = {3},
	urldate = {2019-10-14},
	journal = {AI Magazine},
	author = {Burstein, Jill and Chodorow, Martin and Leacock, Claudia},
	month = sep,
	year = {2004},
	pages = {27--27},
	file = {Full Text PDF:/home/tobias/Zotero/storage/U5TGLYTI/Burstein et al. - 2004 - Automated Essay Evaluation The Criterion Online W.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/XLUETRW4/1774.html:text/html}
}

@article{kohut_effectiveness_1995,
	title = {The {Effectiveness} of {Leading} {Grammar}/{Style} {Software} {Packages} in {Analyzing} {Business} {Students}' {Writing}},
	volume = {9},
	issn = {1050-6519},
	url = {https://doi.org/10.1177/1050651995009003004},
	doi = {10.1177/1050651995009003004},
	abstract = {This study compares the effectiveness of five leading grammar/style analysis software packages in analyzing business students' writing. The software exhibited considerable differences in the following areas: correctly identifying various mechanical and style errors, avoiding annoying and misleading false error messages, and providing helpful remedial advice. No prior research study has empirically compared grammar/style analysis software along all these important dimensions. PowerEdit was found to be the overall superior package, demonsrating proficiency in detecting errors in punctuation, sentence structure, passive voice, and weak wording. The results have significant implications for utilizing grammar/style analysis software to improve students' writing.},
	language = {en},
	number = {3},
	urldate = {2019-10-14},
	journal = {Journal of Business and Technical Communication},
	author = {KOHUT, GARY F. and GORMAN, KEVIN J.},
	month = jul,
	year = {1995},
	pages = {341--361},
	annote = {very old
 }
}

@article{vojak_new_2011,
	title = {New {Spaces} and {Old} {Places}: {An} {Analysis} of {Writing} {Assessment} {Software}},
	volume = {28},
	issn = {8755-4615},
	shorttitle = {New {Spaces} and {Old} {Places}},
	url = {http://www.sciencedirect.com/science/article/pii/S8755461511000259},
	doi = {10.1016/j.compcom.2011.04.004},
	abstract = {This article examines the strengths and weaknesses of emerging writing assessment technologies. Instead of providing a comprehensive review of each program, we take a deliberately selective approach using three key understandings about writing as a framework for analysis: writing is a socially situated activity; writing is functionally and formally diverse; and writing is a meaning-making activity that can be conveyed in multiple modalities. We conclude that the programs available today largely neglect the potential of emerging technologies to promote a broader vision of writing. Instead, they tend to align with the narrow view of writing dominant in a more recent era of testing and accountability, a view that is increasingly thrown into question. New technologies, we conclude, are for the most part being used to reinforce old practices. At a time when computer technology is increasingly looked to as a way to improve assessment, these findings have important implications.},
	number = {2},
	urldate = {2019-10-14},
	journal = {Computers and Composition},
	author = {Vojak, Colleen and Kline, Sonia and Cope, Bill and McCarthey, Sarah and Kalantzis, Mary},
	month = jun,
	year = {2011},
	keywords = {Assessment, Learner feedback, Pedagogy, Technology, Writing},
	pages = {97--111},
	annote = {mentions accuracy on e-rater
 },
	file = {Full Text:/home/tobias/Zotero/storage/E2W48C4U/Vojak et al. - 2011 - New Spaces and Old Places An Analysis of Writing .pdf:application/pdf;ScienceDirect Snapshot:/home/tobias/Zotero/storage/KKK5ZYV6/S8755461511000259.html:text/html}
}

@article{warschauer_automated_2006,
	title = {Automated writing evaluation: defining the classroom research agenda},
	volume = {10},
	issn = {1362-1688},
	shorttitle = {Automated writing evaluation},
	url = {https://doi.org/10.1191/1362168806lr190oa},
	doi = {10.1191/1362168806lr190oa},
	abstract = {With the advent of English as a global language, the ability to write well in English across diverse settings and for different audiences has become an imperative in second language education programmes throughout the world. Yet the teaching of second language writing is often hindered by the great amount of time and skill needed to evaluate repeated drafts of student writing. Online Automated Writing Evaluation programmes have been developed as a way to meet this challenge, and the scoring engines driving such programmes have been analysed in a considerable array of psychometric studies. However, relatively little research has been conducted on how AWE is used in the classroom and the results achieved with such use. In this article, we analyse recent developments in automated writing evaluation, explain the bases on which AWE systems operate, synthesize research with these systems, and propose a multifaceted process/product research programme on the instructional use of AWE. We explore this emerging area of inquiry by proposing a range of potential questions, methodologies and analytical tools that can define such a research agenda.},
	language = {en},
	number = {2},
	urldate = {2019-10-14},
	journal = {Language Teaching Research},
	author = {Warschauer, Mark and Ware, Paige},
	month = apr,
	year = {2006},
	pages = {157--180},
	annote = {maybe related},
	file = {Warschauer and Ware - 2006 - Automated writing evaluation defining the classro.pdf:/home/tobias/Zotero/storage/46Y8NFAR/Warschauer and Ware - 2006 - Automated writing evaluation defining the classro.pdf:application/pdf}
}

@patent{fein_method_2000,
	title = {Method and system for background grammar checking an electronic document},
	url = {https://patents.google.com/patent/US6012075A/en},
	nationality = {US},
	assignee = {Microsoft Corp},
	number = {US6012075A},
	urldate = {2019-10-14},
	author = {Fein, Ronald A. and Krueger, Anthony D.},
	month = jan,
	year = {2000},
	keywords = {document, error, flag, grammar, sentence},
	file = {Fulltext PDF:/home/tobias/Zotero/storage/M8UE98AQ/Fein and Krueger - 2000 - Method and system for background grammar checking .pdf:application/pdf}
}

@inproceedings{burstein_toward_2003,
	address = {Stroudsburg, PA, USA},
	series = {{EACL} '03},
	title = {Toward {Evaluation} of {Writing} {Style}: {Finding} {Overly} {Repetitive} {Word} {Use} in {Student} {Essays}},
	shorttitle = {Toward {Evaluation} of {Writing} {Style}},
	url = {https://doi.org/10.3115/1067807.1067814},
	doi = {10.3115/1067807.1067814},
	abstract = {Automated essay scoring is now an established capability used from elementary school through graduate school for purposes of instruction and assessment. Newer applications provide automated diagnostic feedback about student writing. Feedback includes errors in grammar, usage, and mechanics, comments about writing style, and evaluation of discourse structure. This paper reports on a system that evaluates a characteristic of lower quality essay writing style: repetitious word use. This capability is embedded in a commercial writing assessment application, CriterionSM. The system uses a machine-learning approach with word-based features to model repetitious word use in an essay. System performance well exceeds several baseline algorithms. Agreement between the system and a single human judge exceeds agreement between two human judges.},
	urldate = {2019-10-14},
	booktitle = {Proceedings of the {Tenth} {Conference} on {European} {Chapter} of the {Association} for {Computational} {Linguistics} - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Burstein, Jill and Wolska, Magdalena},
	year = {2003},
	note = {event-place: Budapest, Hungary},
	pages = {35--42},
	annote = {interesting
 },
	file = {ACM Full Text PDF:/home/tobias/Zotero/storage/IBT5A6H8/Burstein and Wolska - 2003 - Toward Evaluation of Writing Style Finding Overly.pdf:application/pdf}
}

@inproceedings{patout_towards_2019,
	address = {New York, NY, USA},
	series = {{EASEAI} 2019},
	title = {Towards {Context}-aware {Automated} {Writing} {Evaluation} {Systems}},
	isbn = {978-1-4503-6852-0},
	url = {http://doi.acm.org/10.1145/3340435.3342722},
	doi = {10.1145/3340435.3342722},
	abstract = {Writing is a crucial skill in our society, which is regularly exerted by students across all disciplines. Automated essay scoring and automatic writing evaluation systems can support professors in the evaluation of written texts and, conversely, help students improving their writing. However, most of those systems fail to consider the context of the writing, such as the targeted audience and the genre. In this paper, we depict our vision towards new-generation AES systems that could evaluate written products while considering their specific context. In education, such tools could support students not only in adapting their written product to their particular context, but also in identifying points for improvement and situational settings where their writing is less proficient.},
	urldate = {2019-10-14},
	booktitle = {Proceedings of the 1st {ACM} {SIGSOFT} {International} {Workshop} on {Education} {Through} {Advanced} {Software} {Engineering} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Patout, Pierre-André and Cordy, Maxime},
	year = {2019},
	note = {event-place: Tallinn, Estonia},
	keywords = {automated writing evaluation, context awareness, education},
	pages = {17--20},
	annote = {rescent important},
	file = {Full Text:/home/tobias/Zotero/storage/NR2INFVJ/Patout and Cordy - 2019 - Towards Context-aware Automated Writing Evaluation.pdf:application/pdf}
}

@inproceedings{wang_case_2011,
	title = {A {Case} {Study} on the {Efficacy} of {Error} {Correction} {Practice} by {Using} the {Automated} {Writing} {Evaluation} {System} {WRM} 2.0 on {Chinese} {College} {Students}' {English} {Writing}},
	doi = {10.1109/ICCIS.2011.21},
	abstract = {A case study was conducted to get insights into the efficacy of error correction practice by using the automated writing evaluation system Writing Roadmap2.0 (WRM), on Chinese college students' English writing. In the research, a group of fresh students in Southwest Petroleum University were required to finish a title- and outline-given essay within 30 minutes by using WRM, and then to revise the essays according to the electronic feedback provided by the system in 15 minutes. By comparing and analyzing the feedback reports given by the system, we found that the participants enjoyed better self-editing ability, writing accuracy and writing quality, but no distinct changes in the organization and ideas. The research indicated that students were able to revise their essays effectively according to the feedback information provided by WRM, but they needed more guidance and advice from teachers if they wanted to improve the structure, coherence and cohesion. This study offers new evidence to support the practice by using automated evaluation system on pedagogical grounds.},
	booktitle = {2011 {International} {Conference} on {Computational} and {Information} {Sciences}},
	author = {Wang, S. and Xian, Y.},
	month = oct,
	year = {2011},
	keywords = {Writing, Accuracy, automated evaluation system, case study, Correlation, Educational institutions, English writing quality, feedback, Organizations, Petroleum, self-revising ability},
	pages = {988--991},
	file = {Full Text:/home/tobias/Zotero/storage/Z9YT7SJU/Wang and Xian - 2011 - A Case Study on the Efficacy of Error Correction P.pdf:application/pdf;IEEE Xplore Abstract Record:/home/tobias/Zotero/storage/Z5N92SME/6086368.html:text/html}
}

@inproceedings{oakman_evolution_1994,
	title = {The evolution of intelligent writing assistants: trends and future prospects},
	shorttitle = {The evolution of intelligent writing assistants},
	doi = {10.1109/TAI.1994.346488},
	abstract = {Since Writer's Workbench (Bell Telephone Laboratories, early 1980's), software for writing assistance and style checking has evolved over the last decade (1984-94) to become more intelligent and interactive. During this period the author has been involved in the development of several software packages and has monitored the growth and sophistication of software solutions. Today certain characteristics have become standard; yet challenges remain. The author discusses trends and suggests areas where we might expect continued future development. Today writers have a variety of useful but limited style and grammar checkers available for most computer systems. Some even come bundled with the current generation of enhanced word processors and are used by many writers. Another approach to writing assistance is suggested by online interactive group writing systems like MediaLink. Possibilities for combining the best features of current systems exist, but further improvements in the quality of the knowledge offered by automated writing assistants will depend on research advances in other areas of natural language processing. The author examines some of these problem areas and suggests approaches from ongoing NLP research that we can expect the writing assistants and style checkers of the future to include among their resources.{\textless}{\textgreater}},
	booktitle = {Proceedings {Sixth} {International} {Conference} on {Tools} with {Artificial} {Intelligence}. {TAI} 94},
	author = {Oakman, R. L.},
	month = nov,
	year = {1994},
	keywords = {Writing, Collaborative work, Computer science, enhanced word processors, future prospects, grammar checkers, intelligent writing assistants, knowledge based systems, Laboratories, MediaLink, Microcomputers, Monitoring, multimedia computing, natural language interfaces, natural language processing, NLP research, online interactive group writing systems, Pattern matching, research advances, software packages, Software packages, style checkers, style checking, Table lookup, Telephony, word processing, writing assistance},
	pages = {233--234},
	file = {IEEE Xplore Abstract Record:/home/tobias/Zotero/storage/WKBG36ZC/metrics.html:text/html}
}

@inproceedings{rubens_solving_2004,
	title = {Solving writing issues related to non-native writers of {English}},
	doi = {10.1109/IPCC.2004.1375272},
	abstract = {Non-native writers of English often have considerable difficulty in creating English language texts. This presents even more problems for scientists and engineers who prepare texts for other subject-area specialists. Typically, solutions to these problems have considered only the nuts-and-bolts approach of 'fixing " the grammar issues. However, the problems are certainly farther reaching in scope, especially for writers of scientific and technical texts. Such issues as tone, specialized terminology, paragraph logic, quantifiers and measurement systems, the writing process, and style and grammar all need to be considered. This work presents the results of several years research involving both academic and professional non-native writers; in doing so, we offer some useful suggestions to help this audience achieve better communication results.},
	booktitle = {International {Professional} {Communication} {Conference}, 2004. {IPCC} 2004. {Proceedings}.},
	author = {Rubens, P. and Southard, S.},
	month = sep,
	year = {2004},
	keywords = {Writing, DNA, Education, English language text, grammar issues, grammars, Logic, measurement systems, Natural languages, nonnative English writers, paragraph logic, Particle measurements, Professional activities, professional communication, Publishing, scientific texts, specialized terminology, subject-area specialists, technical texts, Terminology, writing instruction, writing issues, writing process},
	pages = {42--46},
	annote = {Not directly to do with automatic correction. But we can still learn how to improve.},
	annote = {see if its relevant
 },
	file = {Full Text:/home/tobias/Zotero/storage/5TIZPYYF/Rubens and Southard - 2004 - Solving writing issues related to non-native write.pdf:application/pdf;IEEE Xplore Abstract Record:/home/tobias/Zotero/storage/86TKKIWK/1375272.html:text/html}
}

@article{dodigovic_artificial_2007,
	title = {Artificial {Intelligence} and {Second} {Language} {Learning}: {An} {Efficient} {Approach} to {Error} {Remediation}},
	volume = {16},
	issn = {0965-8416},
	shorttitle = {Artificial {Intelligence} and {Second} {Language} {Learning}},
	url = {https://doi.org/10.2167/la416.0},
	doi = {10.2167/la416.0},
	abstract = {While theoretical approaches to error correction vary in the second language acquisition (SLA) literature, most sources agree that such correction is useful and leads to learning. While some point out the relevance of the communicative context in which the correction takes place, others stress the value of consciousness-raising. Trying to reconcile the two approaches, this paper describes an application of artificial intelligence in the second language error remediation process. The software presented is called the Intelligent Tutor. It diagnoses some typical errors in the writing of university students who are learning English as a second language. A quasi-experimental study consisting of a grammaticality judgment pre-test, a treatment in the form of the Intelligent Tutor and a short answer post-test, was carried out with 266 university students in three countries. The findings show that artificial intelligence is an efficient instrument of error remediation, reducing the error rate by an average of 83\%. This paper discusses the theoretical underpinnings of the software used, briefly describes the software itself and then presents the study design, its findings and their implications in the wider context of second language learning.},
	number = {2},
	urldate = {2019-10-14},
	journal = {Language Awareness},
	author = {Dodigovic, Marina},
	month = may,
	year = {2007},
	keywords = {artificial intelligence, error correction, Intelligent Tutor, remediation, second language learning},
	pages = {99--113},
	annote = {how correction can help to learn
how to design a system},
	file = {Full Text:/home/tobias/Zotero/storage/QUFUX7MT/Dodigovic - 2007 - Artificial Intelligence and Second Language Learni.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/RVJXC4V5/la416.html:text/html}
}

@article{li_rethinking_2015,
	title = {Rethinking the role of automated writing evaluation ({AWE}) feedback in {ESL} writing instruction},
	volume = {27},
	issn = {1060-3743},
	url = {http://www.sciencedirect.com/science/article/pii/S1060374314000757},
	doi = {10.1016/j.jslw.2014.10.004},
	abstract = {The development of language processing technologies and statistical methods has enabled modern automated writing evaluation (AWE) systems to provide feedback on language and content in addition to an automated score. However, concerns have been raised with regard to the instructional and assessment value of AWE in writing classrooms. The findings from a few classroom-based studies concerning the impact of AWE on writing instruction and performance are largely inconclusive. Meanwhile, since research provides favorable evidence for the reliability of AWE corrective feedback, and that writing accuracy is both an important and frustrating issue, it is worthwhile to examine more specifically the impact of AWE corrective feedback on writing accuracy. Therefore, the study used mixed-methods to investigate how Criterion® affected writing instruction and performance. Results suggested that Criterion® has led to increased revisions, and that the corrective feedback from Criterion® helped improve accuracy from a rough to a final draft. The potential benefits were also confirmed by the instructors’ interviews. The students’ perspectives were mixed, but the extent to which the views vary may depend on the students’ language proficiency level and their instructors’ use and perspectives of AWE.},
	urldate = {2019-10-14},
	journal = {Journal of Second Language Writing},
	author = {Li, Jinrong and Link, Stephanie and Hegelheimer, Volker},
	month = mar,
	year = {2015},
	keywords = {AWE, Corrective feedback, ESL writing, Mixed-methods research},
	pages = {1--18},
	file = {Full Text:/home/tobias/Zotero/storage/SEA6JHNI/Li et al. - 2015 - Rethinking the role of automated writing evaluatio.pdf:application/pdf;ScienceDirect Snapshot:/home/tobias/Zotero/storage/DUZQZTFA/S1060374314000757.html:text/html}
}

@article{wang_exploring_2013,
	title = {Exploring the impact of using automated writing evaluation in {English} as a foreign language university students' writing},
	volume = {26},
	issn = {0958-8221},
	url = {https://doi.org/10.1080/09588221.2012.655300},
	doi = {10.1080/09588221.2012.655300},
	abstract = {A period of expanding globalization has emphasized the significant role played by the English language in both communications and information sourcing. In such an environment, it is understandable that many have sought to enhance available writing skills and the assessment of said writing skills. In recent decades, automated writing evaluation (AWE) has been applied with significant frequency to the evaluation and assessment of English writing performance in EFL environments. The purpose of this study is to examine the impact and effect of using AWE on EFL students' writing. In this study, the subjects were 57 EFL freshmen from the Department of Applied English at one university in the south of Taiwan. Both quantitative and qualitative research methods were used to conduct a quasi-experimental research by employing a t-test technique and a semi-structured interview technique, and the nonequivalent groups' pre-test/post-test control and comparison group design were applied to explore the overall effect of using AWE on the improvement of student writing in terms of accuracy, learner autonomy, and interaction. The research results reveal a significant difference between the experimental group and the control group in terms of writing accuracy following the adoption of AWE. Regarding the overall effect and the exploration of students' perceptions toward their usage of the AWE software, it shows that students who used AWE display obvious writing enhancement in terms of writing accuracy and learner autonomy awareness. The pedagogical implications of fully understanding students' usage of AWE in the writing process in order to improve EFL learners' writing performance are fully discussed.},
	number = {3},
	urldate = {2019-10-14},
	journal = {Computer Assisted Language Learning},
	author = {Wang, Ying-Jian and Shang, Hui-Fang and Briody, Paul},
	month = jul,
	year = {2013},
	keywords = {accuracy, automated writing evaluation (AWE), EFL writing, interaction, learner autonomy},
	pages = {234--257},
	annote = {bad title},
	file = {Full Text:/home/tobias/Zotero/storage/GYIYLKLM/Wang et al. - 2013 - Exploring the impact of using automated writing ev.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/PTR4L4RR/09588221.2012.html:text/html}
}

@article{chen_beyond_2008,
	title = {Beyond the design of automated writing evaluation: {Pedagogical} practices and perceived learning effectiveness in {EFL} writing classes},
	volume = {12},
	shorttitle = {Beyond the design of automated writing evaluation},
	number = {2},
	journal = {Language Learning \& Technology},
	author = {Chen, Chi-Fen Emily and Cheng, Wei-Yuan Eugene Cheng},
	year = {2008},
	pages = {94--112},
	file = {Full Text:/home/tobias/Zotero/storage/KCNGAS4P/Chen and Cheng - 2008 - Beyond the design of automated writing evaluation.pdf:application/pdf}
}

@article{grimes_utility_2010,
	title = {Utility in a {Fallible} {Tool}: {A} {Multi}-{Site} {Case} {Study} of {Automated} {Writing} {Evaluation}},
	volume = {8},
	copyright = {Copyright (c)},
	issn = {1540-2525},
	shorttitle = {Utility in a {Fallible} {Tool}},
	url = {https://ejournals.bc.edu/index.php/jtla/article/view/1625},
	abstract = {Automated writing evaluation (AWE) software uses artificial intelligence (AI) to score student essays and support revision. We studied how an AWE program called MY Access!® was used in eight middle schools in Southern California over a three-year period. Although many teachers and students considered automated scoring unreliable, and teachers’ use of AWE was limited by the desire to use conventional writing methods, use of the software still brought important benefits. Observations, interviews, and a survey indicated that using AWE simplified classroom management and increased students’ motivation to write and revise.},
	language = {en},
	number = {6},
	urldate = {2019-10-14},
	journal = {The Journal of Technology, Learning and Assessment},
	author = {Grimes, Douglas and Warschauer, Mark},
	month = mar,
	year = {2010},
	keywords = {automated writing evaluation, artifical intelligence, automated essay scoring, composition, writing},
	file = {Full Text PDF:/home/tobias/Zotero/storage/9J4DUWGR/Grimes and Warschauer - 2010 - Utility in a Fallible Tool A Multi-Site Case Stud.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/5MZXU8AM/1625.html:text/html}
}

@incollection{shermis_automated_2016,
	address = {New York, NY, US},
	title = {Automated writing evaluation: {An} expanding body of knowledge},
	isbn = {978-1-4625-2243-9 978-1-4625-2245-3},
	shorttitle = {Automated writing evaluation},
	abstract = {The chapter on machine scoring for the first edition of this Handbook focused almost exclusively on the use of automated writing evaluation (AWE) technologies to provide information about writing ability in summative assessment contexts. Since that time, AWE has developed its own body of knowledge, building out from its origins in natural language processing and socio cognitive approaches to construct modeling (Shermis, Burstein, \& Bursky, 2013). To demonstrate the depth of contemporary knowledge about AWE, in this chapter we present AWE in terms of the categories of evidence used to demonstrate that these systems are useful in the instruction and assessment of writing. Defining the documentation of such usefulness in terms of validation (Kane, 2006), Williamson, Xi, and Breyer (2012) proposed a unique conceptual framework for AWE focusing on explanation (construct, task, and scoring investigation); evaluation (human and automated score relationships); generalization (generalizability across alternate tasks and test forms); extrapolation (external measure relationships); and utilization (score use and consequences). In an adaptation and reuse of their framework, we discuss the AWE body of knowledge as follows: The Explanation section offers descriptions of how four different AWE systems provide information that may be used to draw relationships between systems and construct relevance; the Evaluation section provides descriptions of the scoring and evaluation functions of AWE systems; the Generalization section discusses empirical studies that use AWE to assess student performance; the Extrapolation section argues for the development of AWE systems that can evaluate writing task types beyond the genre of the academic essay; and the Utilization section focuses on the application of AWE in the context of decision making. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
	booktitle = {Handbook of writing research, 2nd ed},
	publisher = {The Guilford Press},
	author = {Shermis, Mark D. and Burstein, Jill and Elliot, Norbert and Miel, Shayne and Foltz, Peter W.},
	year = {2016},
	keywords = {Computerized Assessment, Scoring (Testing), Writing Skills},
	pages = {395--409},
	annote = {maybe related / not available},
	file = {Snapshot:/home/tobias/Zotero/storage/HCAWJPCF/2015-51875-026.html:text/html}
}

@article{cotos_potential_2011,
	title = {Potential of {Automated} {Writing} {Evaluation} {Feedback}},
	volume = {28},
	issn = {0742-7778},
	url = {https://www.jstor.org/stable/calicojournal.28.2.420},
	abstract = {ABSTRACT This paper presents an empirical evaluation of automated writing evaluation (AWE) feedback used for L2 academic writing teaching and learning. It introduces the Intelligent Academic Discourse Evaluator (IADE), a new web-based AWE program that analyzes the introduction section to research articles and generates immediate, individualized, and discipline-specific feedback. The purpose of the study was to investigate the potential of IADE's feedback. A mixed-methods approach with a concurrent transformative strategy was employed. Quantitative data consisted of responses to Likert-scale, yes/no, and open-ended survey questions; automated and human scores for first and final drafts; and pre-/posttest scores. Qualitative data contained students' first and final drafts as well as transcripts of think-aloud protocols and Camtasia computer screen recordings, observations, and semistructured interviews. The findings indicate that IADE's colorcoded and numerical feedback possesses potential for facilitating language learning, a claim supported by evidence of focus on discourse form, noticing of negative evidence, improved rhetorical quality of writing, and increased learning gains.},
	number = {2},
	urldate = {2019-10-14},
	journal = {CALICO Journal},
	author = {Cotos, Elena},
	year = {2011},
	pages = {420--459},
	file = {Cotos - 2011 - Potential of Automated Writing Evaluation Feedback.pdf:/home/tobias/Zotero/storage/HK2AU7AR/Cotos - 2011 - Potential of Automated Writing Evaluation Feedback.pdf:application/pdf}
}

@article{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and signiﬁcantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	language = {en},
	urldate = {2019-10-14},
	journal = {arXiv:1802.05365 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05365},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready},
	annote = {from Ela
 },
	file = {Peters et al. - 2018 - Deep contextualized word representations.pdf:/home/tobias/Zotero/storage/RAW2Y62R/Peters et al. - 2018 - Deep contextualized word representations.pdf:application/pdf}
}

@article{bell_context_2019,
	title = {Context is {Key}: {Grammatical} {Error} {Detection} with {Contextual} {Word} {Representations}},
	shorttitle = {Context is {Key}},
	url = {http://arxiv.org/abs/1906.06593},
	abstract = {Grammatical error detection (GED) in nonnative writing requires systems to identify a wide range of errors in text written by language learners. Error detection as a purely supervised task can be challenging, as GED datasets are limited in size and the label distributions are highly imbalanced. Contextualized word representations offer a possible solution, as they can efﬁciently capture compositional information in language and can be optimized on large amounts of unsupervised data. In this paper, we perform a systematic comparison of ELMo, BERT and Flair embeddings (Peters et al., 2017; Devlin et al., 2018; Akbik et al., 2018) on a range of public GED datasets, and propose an approach to effectively integrate such representations in current methods, achieving a new state of the art on GED. We further analyze the strengths and weaknesses of different contextual embeddings for the task at hand, and present detailed analyses of their impact on different types of errors.},
	language = {en},
	urldate = {2019-10-14},
	journal = {arXiv:1906.06593 [cs]},
	author = {Bell, Samuel and Yannakoudakis, Helen and Rei, Marek},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.06593},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {from Ela},
	file = {Bell et al. - 2019 - Context is Key Grammatical Error Detection with C.pdf:/home/tobias/Zotero/storage/TQI8T9XS/Bell et al. - 2019 - Context is Key Grammatical Error Detection with C.pdf:application/pdf}
}

@article{cavaleri_you_2016,
	title = {You want me to check your grammar again? {The} usefulness of an online grammar checker as perceived by students},
	volume = {10},
	shorttitle = {You want me to check your grammar again?},
	number = {1},
	journal = {Journal of Academic Language and Learning},
	author = {Cavaleri, Michelle Rose and Dianati, Saib},
	year = {2016},
	pages = {A223--A236},
	annote = {overview of Grammarly
trial at two colleges
doing survey according to usefulness
also reviews the acceptance by students},
	file = {Cavaleri and Dianati - 2016 - You want me to check your grammar again The usefu.pdf:/home/tobias/Zotero/storage/DK2JJ7IG/Cavaleri and Dianati - 2016 - You want me to check your grammar again The usefu.pdf:application/pdf}
}

@article{manchanda_various_2016,
	title = {Various techniques used for grammar checking},
	volume = {9},
	number = {1},
	journal = {International Journal of Computer Applications \& Information Technology},
	author = {Manchanda, Blossom and Athavale, Vijay Anant and kumar Sharma, Sanjeev},
	year = {2016},
	pages = {177},
	annote = {comparison of three different techniques (rule basec, statisctica, syntax)
 },
	file = {Full Text:/home/tobias/Zotero/storage/7VWZMZQ8/Manchanda et al. - 2016 - Various techniques used for grammar checking.pdf:application/pdf}
}

@article{qassemzadeh_impact_2016,
	title = {The {Impact} of {Feedback} {Provision} by {Grammarly} {Software} and {Teachers} on {Learning} {Passive} {Structures} by {Iranian} {EFL} {Learners}},
	volume = {6},
	copyright = {Copyright (c) 2016 Academy Publication},
	issn = {1799-2591},
	url = {http://www.academypublication.com/ojs/index.php/tpls/article/view/tpls060918841894},
	doi = {10.17507/tpls.0609.23},
	abstract = {A major concern in today's world of pedagogy in general and language teaching, in particular, is the application of computer-assisted learning to improve students' achievement. There has been a long time that in the classroom setting only the teacher's feedback in a traditional way has been used in teaching. Due to the fact that this kind of notion can be traced back to a traditional attitude toward feedback, we looked for a new alternative in order to bring some innovation in an educational environment, namely Grammarly Software feedback provision. Therefore, the aim of the present study was to explore the impact of feedback provision by Grammarly Software and teachers on learning passive structures by EFL learners. Through convenience sampling, 70 intermediate male and female EFL learners were selected, then they were randomly assigned to two main groups: the experimental and control group. A grammar pre-test, a post-test, and a delayed post-test were administrated to the participants in six sessions. The results of the data gathered from pre-test and post-test reveal that the effect of teacher on learning passive structure, in pre-test and post-test, were more than the effect of Grammarly Software on learning passive structure of the learners, and the effect of Grammarly Software on learning passive structure in delayed post-test scores was more than the effect of teacher on learning passive structure of learners. The results might have implications for language teachers, learners, and materials developers.},
	language = {en},
	number = {9},
	urldate = {2019-10-23},
	journal = {Theory and Practice in Language Studies},
	author = {Qassemzadeh, Abolfazl and Soleimani, Hassan},
	month = sep,
	year = {2016},
	keywords = {grammarly software, passive structure, software feedback, teacher's feedback},
	pages = {1884--1894},
	annote = {quite useless
 },
	file = {Full Text PDF:/home/tobias/Zotero/storage/VU3LGRMN/Qassemzadeh and Soleimani - 2016 - The Impact of Feedback Provision by Grammarly Soft.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/8ABS9QQX/tpls060918841894.html:text/html}
}

@article{nova_utilizing_2018,
	title = {{UTILIZING} {GRAMMARLY} {IN} {EVALUATING} {ACADEMIC} {WRITING}: {A} {NARRATIVE} {RESEARCH} {ON} {EFL} {STUDENTS}’ {EXPERIENCE}},
	volume = {7},
	copyright = {Copyright (c) 2018 Muhamad Nova},
	issn = {2442-482X},
	shorttitle = {{UTILIZING} {GRAMMARLY} {IN} {EVALUATING} {ACADEMIC} {WRITING}},
	url = {http://ojs.fkip.ummetro.ac.id/index.php/english/article/view/1332},
	doi = {10.24127/pj.v7i1.1332},
	abstract = {With the development of technology, any writer now can easily check their academic writing with automated writing evaluation program. Though, the utilization of this program may bring both benefits and drawbacks. Thus, a consideration of its strengths and weaknesses is needed. To fill the need, this study aimed to identify the strengths and weaknesses of Grammarly program as an automated writing evaluation program in evaluating academic writing. Using a narrative inquiry in exploring three Indonesian postgraduate students’ experiences by conducting interview and documentation, the result showed that this program has provided useful color-coded feedback with explanation and example, ease of account access, high rate of evaluation speed, and free service for evaluating academic writing. However, some caveats were also found in this program utilization, such as several misleading feedbacks, weaknesses on detecting the type of English and reference list, and lack of context and content evaluation experienced, which became the weaknesses of this program. Further investigation on the efficiency of the feedback given by Grammarly in improving students’ writing quality is needed.},
	language = {en},
	number = {1},
	urldate = {2019-10-23},
	journal = {Premise: Journal of English Education},
	author = {Nova, Muhamad},
	month = apr,
	year = {2018},
	keywords = {automated writing evaluation, academic writing, experience, Grammarly},
	pages = {80--97},
	file = {Full Text PDF:/home/tobias/Zotero/storage/C444MJV5/Nova - 2018 - UTILIZING GRAMMARLY IN EVALUATING ACADEMIC WRITING.pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/UIWABZT4/1332.html:text/html}
}

@techreport{ventayen_graduate_2018,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Graduate {Students}’ {Perspective} on the {Usability} of {Grammarly}® in {One} {ASEAN} {State} {University}},
	url = {https://papers.ssrn.com/abstract=3310702},
	abstract = {One of the criteria for excellent work is a perfect grammar in English, which is the global lingua franca. The Pangasinan State University subscribed to Grammarly® software which considered as one of the leading grammar checker and plagiarism tester software available. The objective of the paper is to evaluate the software based on the perception of the graduate students from the Graduate Students of PSU Open University Systems. The selection of participants is purposive where 20\% of the total graduate students of the Batch 2018 who completed there thesis writing before graduation was selected as the respondents. A SUS questionnaire and follow up the interview as part of the triangulation method was used in order to determine the usability of the software, its strengths, and weakness. This study also focuses on the impact of the application for Non-English majors. Based on the result of the study, the majority of the respondents agree that the utilized software is usable. The users identified the strength of the software that helps the user improve writing such as automatic detection of mistakes in Conceptual Writing, Grammar, Punctuation, Sentence Structure, Style and Vocabulary Enhancement. While other users also identified some weakness for possible future improvement of the software, the overall result shows that there is a significant improvement in writing for English and non-English majors. Students who are not English majors showed that there is a significant change of confidence level in writing. It recommends that the software is continuously utilized. It is also recommended that the software should improve its detection to avoid misleading feedback for users.},
	language = {en},
	number = {ID 3310702},
	urldate = {2019-10-23},
	institution = {Social Science Research Network},
	author = {Ventayen, Randy Joy Magno and Orlanda-Ventayen, Caren C.},
	month = aug,
	year = {2018},
	keywords = {grammar checker, grammarly, plagiarism test},
	file = {Snapshot:/home/tobias/Zotero/storage/3BKVPHZX/papers.html:text/html;Ventayen and Orlanda-Ventayen - 2018 - Graduate Students’ Perspective on the Usability of.pdf:/home/tobias/Zotero/storage/NETC3E9F/Ventayen and Orlanda-Ventayen - 2018 - Graduate Students’ Perspective on the Usability of.pdf:application/pdf}
}

@misc{noauthor_grammarly_nodate,
	title = {Grammarly},
	url = {https://www.grammarly.com/},
	abstract = {Grammarly makes sure everything you type is easy to read, effective, and mistake-free. Try it today:},
	urldate = {2019-11-27},
	file = {Snapshot:/home/tobias/Zotero/storage/D36J3T9Y/www.grammarly.com.html:text/html}
}

@article{davis_user_1989,
	title = {User {Acceptance} of {Computer} {Technology}: {A} {Comparison} of {Two} {Theoretical} {Models}},
	volume = {35},
	issn = {0025-1909},
	shorttitle = {User {Acceptance} of {Computer} {Technology}},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.35.8.982},
	doi = {10.1287/mnsc.35.8.982},
	abstract = {Computer systems cannot improve organizational performance if they aren't used. Unfortunately, resistance to end-user systems by managers and professionals is a widespread problem. To better predict, explain, and increase user acceptance, we need to better understand why people accept or reject computers. This research addresses the ability to predict peoples' computer acceptance from a measure of their intentions, and the ability to explain their intentions in terms of their attitudes, subjective norms, perceived usefulness, perceived ease of use, and related variables. In a longitudinal study of 107 users, intentions to use a specific system, measured after a one-hour introduction to the system, were correlated 0.35 with system use 14 weeks later. The intention-usage correlation was 0.63 at the end of this time period. Perceived usefulness strongly influenced peoples' intentions, explaining more than half of the variance in intentions at the end of 14 weeks. Perceived ease of use had a small but significant effect on intentions as well, although this effect subsided over time. Attitudes only partially mediated the effects of these beliefs on intentions. Subjective norms had no effect on intentions. These results suggest the possibility of simple but powerful models of the determinants of user acceptance, with practical value for evaluating systems and guiding managerial interventions aimed at reducing the problem of underutilized computer technology.},
	number = {8},
	urldate = {2019-12-05},
	journal = {Management Science},
	author = {Davis, Fred D. and Bagozzi, Richard P. and Warshaw, Paul R.},
	month = aug,
	year = {1989},
	pages = {982--1003}
}

@article{zhang_chengqing_2014,
	title = {Chengqing {Zong}: {Statistical} natural language processing (second edition)},
	volume = {28},
	issn = {1573-0573},
	url = {https://doi.org/10.1007/s10590-014-9150-3},
	doi = {10.1007/s10590-014-9150-3},
	number = {2},
	journal = {Machine Translation},
	author = {Zhang, Xiaojun},
	month = oct,
	year = {2014},
	pages = {155--158},
	file = {Full Text:/home/tobias/Zotero/storage/7Q8Z7GV9/Zhang - 2014 - Chengqing Zong Statistical natural language proce.pdf:application/pdf}
}

@article{macleod_large_nodate,
	title = {A {Large} {Syntactic} {Dictionary} for {Natural} {Language} {Processing}},
	abstract = {This article is a detailed account of COMLEX Syntax, an on-line syntactic dictionary of English, developed by the Proteus Project at New York University under the auspices of the Linguistics Data Consortium. This lexicon was intended to be used for a variety of tasks in natural language processing by computer and as such has very detailed classes with a large number of syntactic features and complements for the major parts of speech and is, as far as possible, theory neutral. The dictionary was entered by hand with reference to hard copy dictionaries, an on-line concordance and native speakers’ intuition. Thus it is without prior encumbrances and can be used for both pure research and commercial purposes.},
	language = {en},
	author = {Macleod, Catherine and Grishman, Ralph and Meyers, Adam},
	pages = {23},
	file = {Macleod et al. - A Large Syntactic Dictionary for Natural Language .pdf:/home/tobias/Zotero/storage/7KC2VZWA/Macleod et al. - A Large Syntactic Dictionary for Natural Language .pdf:application/pdf}
}

@article{vandeventer_creating_2001,
	title = {Creating a grammar checker for {CALL} by constraint relaxation: a feasibility study},
	volume = {13},
	issn = {1474-0109, 0958-3440},
	shorttitle = {Creating a grammar checker for {CALL} by constraint relaxation},
	url = {https://www.cambridge.org/core/journals/recall/article/creating-a-grammar-checker-for-call-by-constraint-relaxation-a-feasibility-study/11979C1FE2584B9B8815162CE3E127AE},
	doi = {10.1017/S095834400100101X},
	abstract = {Intelligent feedback on learners’ full written
sentence productions requires the use of Natural Language Processing (NLP)
tools and, in particular, of a diagnosis system. Most syntactic parsers, on which
grammar checkers are based, are designed to parse grammatical sentences and/or
native speaker productions. They are therefore not necessarily suitable for
language learners. In this paper, we concentrate on the transformation of a
French syntactic parser into a grammar checker geared towards intermediate to
advanced learners of French. Several techniques are envisaged to allow the
parser to handle ill-formed input, including constraint relaxation. By the
very nature of this technique, parsers can generate complete analyses for
ungrammatical sentences. Proper labelling of where the analysis has been able to
proceed thanks to a specific constraint relaxation forms the basis of the error
diagnosis. Parsers with relaxed constraints tend to produce more complete,
although incorrect, analyses for grammatical sentences, and several complete
analyses for ungrammatical sentences. This increased number of analyses per
sentence has one major drawback: it slows down the system and requires more
memory. An experiment was conducted to observe the behaviour of our parser in the
context of constraint relaxation. Three specific constraints, agreement in number,
gender, and person, were selected and relaxed in different combinations. A learner
corpus was parsed with each combination. The evolution of the number of correct
diagnoses and of parsing speed, among other factors, were monitored. We then
evaluated, by comparing the results, whether large scale constraint relaxation is
a viable option to transform our syntactic parser into an efficient grammar
checker for CALL.},
	language = {en},
	number = {1},
	urldate = {2019-12-09},
	journal = {ReCALL},
	author = {Vandeventer, Anne},
	month = may,
	year = {2001},
	pages = {110--120},
	file = {Full Text PDF:/home/tobias/Zotero/storage/GS9UFTJM/Vandeventer - 2001 - Creating a grammar checker for CALL by constraint .pdf:application/pdf;Snapshot:/home/tobias/Zotero/storage/LHLNJGDA/11979C1FE2584B9B8815162CE3E127AE.html:text/html}
}

@misc{noauthor_pos_2018,
	title = {{POS} tags and part-of-speech tagging {\textbar} {Sketch} {Engine}},
	url = {https://www.sketchengine.eu/pos-tags/},
	abstract = {A POS tag (part-of-speech tag) is a label showing the part of speech of each token (word) in a text corpus. POS tags are assigned automatically by a POS tagger.},
	language = {en-GB},
	urldate = {2019-12-09},
	month = mar,
	year = {2018},
	file = {Snapshot:/home/tobias/Zotero/storage/SLF3VEA7/pos-tags.html:text/html}
}

@article{gamon_using_2009,
	title = {Using {Statistical} {Techniques} and {Web} {Search} to {Correct} {ESL} {Errors}},
	volume = {26},
	issn = {0742-7778},
	url = {www.jstor.org/stable/calicojournal.26.3.491},
	abstract = {Abstract In this paper we present a system for automatic correction of errors made by learners of English. The system has two novel aspects. First, machine-learned classifiers trained on large amounts of native data and a very large language model are combined to optimize the precision of suggested corrections. Second, the user can access real-life web examples of both their original formulation and the suggested correction. We discuss technical details of the system, including the choice of classifier, feature sets, and language model. We also present results from an evaluation of the system on a set of corpora. We perform an automatic evaluation on native English data and a detailed manual analysis of performance on three corpora of nonnative writing: the Chinese Learners' of English Corpus (CLEC) and two corpora of web and email writing.},
	number = {3},
	urldate = {2019-12-09},
	journal = {CALICO Journal},
	author = {Gamon, Michael and Leacock, Claudia and Brockett, Chris and Dolan, William B. and Gao, Jianfeng and Belenko, Dmitriy and Klementiev, Alexandre},
	year = {2009},
	pages = {491--511}
}

@patent{kantrowitz_method_2003,
	title = {Method for rule-based correction of spelling and grammar errors},
	url = {https://patents.google.com/patent/US6618697B1/en},
	nationality = {US},
	assignee = {Justsystem Corp},
	number = {US6618697B1},
	urldate = {2019-12-09},
	author = {Kantrowitz, Mark and Baluja, Shumeet},
	month = sep,
	year = {2003},
	keywords = {errors, rules, spelling, word, words},
	file = {Fulltext PDF:/home/tobias/Zotero/storage/AL9KC6WR/Kantrowitz and Baluja - 2003 - Method for rule-based correction of spelling and g.pdf:application/pdf}
}

@article{lim_review_2012,
	title = {Review of {Criterion} for {English} {Language} {Learning}},
	volume = {16},
	number = {2},
	journal = {Language Learning \& Technology},
	author = {Lim, Hyojung and Kahng, Jimin},
	year = {2012},
	pages = {38--45},
	annote = {must read
 },
	file = {Full Text:/home/tobias/Zotero/storage/ACQNUBR5/Lim and Kahng - 2012 - Review of Criterion for English Language Learning.pdf:application/pdf}
}

@misc{noauthor_ets_nodate,
	title = {{ETS} {Criterion} writing evaluation service},
	url = {https://criterion.ets.org/criterion/default.aspx},
	urldate = {2019-12-10},
	file = {ETS Criterion writing evaluation service:/home/tobias/Zotero/storage/NXIRAYQP/default.html:text/html}
}

@misc{noauthor_about_nodate,
	title = {About the e-rater {Scoring} {Engine}},
	url = {https://www.ets.org/erater/about},
	urldate = {2019-12-10},
	file = {About the e-rater Scoring Engine:/home/tobias/Zotero/storage/5JCCTC3K/about.html:text/html}
}

@article{llosa_comparability_2019,
	title = {Comparability of students’ writing performance on {TOEFL} {iBT} and in required university writing courses},
	volume = {36},
	issn = {0265-5322},
	url = {https://doi.org/10.1177/0265532218763456},
	doi = {10.1177/0265532218763456},
	abstract = {Investigating the comparability of students’ performance on TOEFL writing tasks and actual academic writing tasks is essential to provide backing for the extrapolation inference in the TOEFL validity argument (Chapelle, Enright, \& Jamieson, 2008). This study compared 103 international non-native-English-speaking undergraduate students’ performance on two TOEFL iBT® writing tasks with their performance in required writing courses in US universities as measured by instructors’ ratings of student proficiency, instructor-assigned grades on two course assignments, and five dimensions of writing quality of the first and final drafts of those course assignments: grammatical, cohesive, rhetorical, sociopragmatic, and content control. Also, the quality of the writing on the TOEFL writing tasks was compared with the first and final drafts of responses to written course assignments using a common analytic rubric along the five dimensions. Correlations of scores from TOEFL tasks (Independent, Integrated, and the total Writing section) with instructor ratings of students’ overall English proficiency and writing proficiency were moderate and significant. However, only scores on the Integrated task and the Writing section were correlated with instructor-assigned grades on course assignments. Correlations between scores on TOEFL tasks and all dimensions of writing quality were positive and significant, though of lower magnitude for final drafts than for first drafts. The TOEFL scores were most highly correlated with cohesive and grammatical control and had the lowest correlations with rhetorical organization. The quality of the writing on the TOEFL tasks was comparable to that of the first drafts of course assignment but not the final drafts. These findings provide backing for the extrapolation inference, suggesting that the construct of academic writing proficiency as assessed by TOEFL “accounts for the quality of linguistic performance in English-medium institutions of higher education” (Chapelle, Enright, \& Jamieson, 2008, p. 21).},
	language = {en},
	number = {2},
	urldate = {2019-12-10},
	journal = {Language Testing},
	author = {Llosa, Lorena and Malone, Margaret E.},
	month = apr,
	year = {2019},
	pages = {235--263}
}

@article{weigle_validation_2010,
	title = {Validation of {Automated} {Scores} of {TOEFL} {iBT} {Tasks} against {Non}-{Test} {Indicators} of {Writing} {Ability}},
	volume = {27},
	issn = {0265-5322},
	doi = {10.1177/0265532210364406},
	abstract = {Automated scoring has the potential to dramatically reduce the time and costs associated with the assessment of complex skills such as writing, but its use must be validated against a variety of criteria for it to be accepted by test users and stakeholders. This study approaches validity by comparing human and automated scores on responses to TOEFL[R] iBT Independent writing tasks with several non-test indicators of writing ability: student self-assessment, instructor assessment, and independent ratings of non-test writing samples. Automated scores were produced using "e-rater"[R], developed by Educational Testing Service (ETS). Correlations between both human and "e-rater" scores and non-test indicators were moderate but consistent, providing criterion-related validity evidence for the use of "e-rater" along with human scores. The implications of the findings for the validity of automated scores are discussed. (Contains 10 tables and 2 notes.)},
	language = {en},
	number = {3},
	urldate = {2019-12-10},
	journal = {Language Testing},
	author = {Weigle, Sara Cushing},
	month = jul,
	year = {2010},
	keywords = {Correlation, Computer Assisted Testing, English (Second Language), Language Tests, Scores, Second Language Learning, Self Evaluation (Individuals), Validity, Writing Ability},
	pages = {335--353},
	file = {Snapshot:/home/tobias/Zotero/storage/46S8J9ZJ/eric.ed.gov.html:text/html}
}

@misc{noauthor_fhnw_nodate,
	title = {{FHNW}},
	url = {https://www.fhnw.ch/en},
	urldate = {2019-12-12},
	file = {University of Applied Sciences and Arts Northwestern Switzerland | FHNW:/home/tobias/Zotero/storage/S2GZXT6F/en.html:text/html}
}